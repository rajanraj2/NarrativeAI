{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "universal-sentence-encoder is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 185\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    184\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the event/topic: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m     \u001b[43msummarize_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 145\u001b[0m, in \u001b[0;36msummarize_event\u001b[1;34m(topic)\u001b[0m\n\u001b[0;32m    142\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m [preprocess_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m combined_data]\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Perform topic modeling\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m topics, topic_words, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Extract entities and sentiment for each news and reddit post\u001b[39;00m\n\u001b[0;32m    148\u001b[0m events \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[4], line 106\u001b[0m, in \u001b[0;36mget_topics\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(tagged_data, total_examples\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcorpus_count, epochs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Use Top2Vec after training Doc2Vec\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m top2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mTop2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muniversal-sentence-encoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m topics, topic_words, word_scores, topic_scores \u001b[38;5;241m=\u001b[39m top2vec_model\u001b[38;5;241m.\u001b[39mget_topics()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m topics, topic_words, word_scores, topic_scores\n",
      "File \u001b[1;32mc:\\Users\\rans2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\top2vec\\Top2Vec.py:608\u001b[0m, in \u001b[0;36mTop2Vec.__init__\u001b[1;34m(self, documents, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, gpu_umap, hdbscan_args, gpu_hdbscan, index_topics, verbose)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m embedding_model\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_import_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPre-processing documents for training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    612\u001b[0m \u001b[38;5;66;03m# preprocess documents\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rans2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\top2vec\\Top2Vec.py:1134\u001b[0m, in \u001b[0;36mTop2Vec._check_import_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;129;01min\u001b[39;00m use_models:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAVE_TENSORFLOW:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not available.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1135\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: pip install top2vec[sentence_encoders]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;129;01min\u001b[39;00m sbert_models:\n\u001b[0;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAVE_TORCH:\n",
      "\u001b[1;31mImportError\u001b[0m: universal-sentence-encoder is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import praw\n",
    "import spacy\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from top2vec import Top2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Set up API keys\n",
    "NEWS_API_KEY = 'a8bcc3d808424f098069111c114e3ff8'\n",
    "reddit = praw.Reddit(client_id='pC9dZB_wqZ9Son7HPJUc8w',\n",
    "                     client_secret='4hpR2C7Fpov0QGM7g5QNKJqsN0SmRA',\n",
    "                     user_agent='event_summary_app')\n",
    "\n",
    "# Function to fetch news from NewsAPI\n",
    "def get_news(keyword, page_size=20):\n",
    "    url = f'https://newsapi.org/v2/everything?q={keyword}&apiKey={NEWS_API_KEY}&pageSize={page_size}'\n",
    "    response = requests.get(url)\n",
    "    articles = response.json().get('articles', [])\n",
    "    \n",
    "    news = []\n",
    "    for article in articles:\n",
    "        news.append({\n",
    "            'title': article['title'],\n",
    "            'description': article['description'],\n",
    "            'url': article['url'],\n",
    "            'publishedAt': article['publishedAt']\n",
    "        })\n",
    "    return news\n",
    "\n",
    "# Function to fetch top Reddit posts\n",
    "def fetch_top_posts(search_query, subreddit_name='all', limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    top_posts = subreddit.search(search_query, sort='top', limit=limit)\n",
    "    \n",
    "    post_data = []\n",
    "    for post in top_posts:\n",
    "        post_datetime = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        upvotes = post.ups\n",
    "        downvotes = upvotes - post.score\n",
    "        \n",
    "        awards = post.all_awards if hasattr(post, 'all_awards') else []\n",
    "        award_names = [award['name'] for award in awards]\n",
    "        \n",
    "        flair = post.link_flair_text if post.link_flair_text else \"No Flair\"\n",
    "        \n",
    "        post_data.append({\n",
    "            'Title': post.title,\n",
    "            'Body': post.selftext,\n",
    "            'Date': post_datetime,\n",
    "            'Upvotes': upvotes,\n",
    "            'Downvotes': downvotes,\n",
    "            'Comments': post.num_comments,\n",
    "            'Link': post.url,\n",
    "            'Awards': award_names,\n",
    "            'Flair': flair\n",
    "        })\n",
    "    \n",
    "    return post_data\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Named Entity Recognition\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Sentiment analysis\n",
    "sentiment_analyzer = pipeline('sentiment-analysis')\n",
    "def analyze_sentiment(text):\n",
    "    return sentiment_analyzer(text)\n",
    "\n",
    "# # Topic Modeling with Top2Vec\n",
    "# def get_topics(documents):\n",
    "#     # model = Top2Vec(documents, embedding_model='universal-sentence-encoder')\n",
    "#     model = Top2Vec(documents, embedding_model='doc2vec')\n",
    "#     topics, topic_words, word_scores, topic_scores = model.get_topics()\n",
    "#     return topics, topic_words, word_scores, topic_scores\n",
    "\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def get_topics(documents):\n",
    "    tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(documents)]\n",
    "    \n",
    "    # Initialize and build vocabulary\n",
    "    model = Doc2Vec(vector_size=100, min_count=2, epochs=40, workers=4)\n",
    "    model.build_vocab(tagged_data)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    # Use Top2Vec after training Doc2Vec\n",
    "    top2vec_model = Top2Vec(documents, embedding_model='universal-sentence-encoder')\n",
    "    \n",
    "    topics, topic_words, word_scores, topic_scores = top2vec_model.get_topics()\n",
    "    return topics, topic_words, word_scores, topic_scores\n",
    "\n",
    "# Event Timeline Plotting\n",
    "def plot_timeline(events):\n",
    "    event_dates = [datetime.strptime(event['date'], \"%Y-%m-%d %H:%M\") for event in events]\n",
    "    event_texts = [event['text'] for event in events]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(event_dates, [i for i in range(len(event_dates))], 'bo-')\n",
    "    \n",
    "    for i, text in enumerate(event_texts):\n",
    "        plt.text(event_dates[i], i, text, fontsize=9, verticalalignment='bottom', horizontalalignment='right')\n",
    "    \n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.title(\"Event Timeline\")\n",
    "    plt.xlabel(\"Date and Time\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main function to combine everything\n",
    "def summarize_event(topic):\n",
    "    # Fetch news and reddit data\n",
    "    news_articles = get_news(topic)\n",
    "    # reddit_posts = fetch_top_posts(topic)\n",
    "    \n",
    "    # Combine and preprocess data\n",
    "    combined_data = [article['description'] for article in news_articles if article['description']] \n",
    "    # + \\\n",
    "    #                 [post['Body'] for post in reddit_posts if post['Body']]\n",
    "\n",
    "    cleaned_data = [preprocess_text(text) for text in combined_data]\n",
    "    \n",
    "    # Perform topic modeling\n",
    "    topics, topic_words, _, _ = get_topics(cleaned_data)\n",
    "    \n",
    "    # Extract entities and sentiment for each news and reddit post\n",
    "    events = []\n",
    "    for article in news_articles:\n",
    "        sentiment = analyze_sentiment(article['description'])\n",
    "        entities = extract_entities(article['description'])\n",
    "        events.append({\n",
    "            'text': article['title'],\n",
    "            'sentiment': sentiment,\n",
    "            'entities': entities,\n",
    "            'source': article['url'],\n",
    "            'date': article['publishedAt']\n",
    "        })\n",
    "    \n",
    "    for post in reddit_posts:\n",
    "        sentiment = analyze_sentiment(post['Body'])\n",
    "        entities = extract_entities(post['Body'])\n",
    "        events.append({\n",
    "            'text': post['Title'],\n",
    "            'sentiment': sentiment,\n",
    "            'entities': entities,\n",
    "            'source': post['Link'],\n",
    "            'date': post['Date']\n",
    "        })\n",
    "    \n",
    "    # Plot timeline\n",
    "    plot_timeline(events)\n",
    "    \n",
    "    # Print structured summary\n",
    "    print(\"Summary of Events:\")\n",
    "    for event in events:\n",
    "        print(f\"- {event['text']} (Source: {event['source']}, Date: {event['date']})\")\n",
    "        print(f\"  Sentiment: {event['sentiment'][0]['label']} ({event['sentiment'][0]['score']:.2f})\")\n",
    "        print(f\"  Key Entities: {', '.join([ent[0] for ent in event['entities']])}\")\n",
    "        print()\n",
    "\n",
    "# Input from user\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter the event/topic: \")\n",
    "    summarize_event(topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
