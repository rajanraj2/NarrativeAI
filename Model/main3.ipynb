{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 169\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    168\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the event/topic: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 169\u001b[0m     \u001b[43msummarize_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 130\u001b[0m, in \u001b[0;36msummarize_event\u001b[1;34m(topic)\u001b[0m\n\u001b[0;32m    127\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m [preprocess_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m combined_data]\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Perform topic modeling\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m topics, topic_words, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Extract entities and sentiment for each news and reddit post\u001b[39;00m\n\u001b[0;32m    133\u001b[0m events \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[3], line 89\u001b[0m, in \u001b[0;36mget_topics\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m     87\u001b[0m model \u001b[38;5;241m=\u001b[39m Doc2Vec(vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild_vocab(tagged_data)\n\u001b[1;32m---> 89\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagged_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Use Top2Vec for topic modeling\u001b[39;00m\n\u001b[0;32m     92\u001b[0m top2vec_model \u001b[38;5;241m=\u001b[39m Top2Vec(documents, embedding_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc2vec\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rans2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\doc2vec.py:516\u001b[0m, in \u001b[0;36mDoc2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffsets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m offsets\n\u001b[0;32m    514\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_doctags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m start_doctags\n\u001b[1;32m--> 516\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDoc2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rans2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\word2vec.py:1045\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha \u001b[38;5;241m=\u001b[39m end_alpha \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m-> 1045\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_training_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1050\u001b[0m     msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     ),\n\u001b[0;32m   1055\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rans2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\word2vec.py:1554\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffective \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m higher than previous training cycles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mkey_to_index:  \u001b[38;5;66;03m# should be set by `build_vocab`\u001b[39;00m\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must first build vocabulary before training the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors):\n\u001b[0;32m   1556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must initialize vectors before training the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import praw\n",
    "import spacy\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from top2vec import Top2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up API keys (secure these in a real application)\n",
    "NEWS_API_KEY = 'your_news_api_key'\n",
    "reddit = praw.Reddit(client_id='your_client_id',\n",
    "                     client_secret='your_client_secret',\n",
    "                     user_agent='event_summary_app')\n",
    "\n",
    "# Function to fetch news from NewsAPI\n",
    "def get_news(keyword, page_size=20):\n",
    "    url = f'https://newsapi.org/v2/everything?q={keyword}&apiKey={NEWS_API_KEY}&pageSize={page_size}'\n",
    "    response = requests.get(url)\n",
    "    articles = response.json().get('articles', [])\n",
    "    \n",
    "    news = []\n",
    "    for article in articles:\n",
    "        news.append({\n",
    "            'title': article['title'],\n",
    "            'description': article['description'],\n",
    "            'url': article['url'],\n",
    "            'publishedAt': article['publishedAt']\n",
    "        })\n",
    "    return news\n",
    "\n",
    "# Function to fetch top Reddit posts\n",
    "def fetch_top_posts(search_query, subreddit_name='all', limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    top_posts = subreddit.search(search_query, sort='top', limit=limit)\n",
    "    \n",
    "    post_data = []\n",
    "    for post in top_posts:\n",
    "        post_datetime = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        upvotes = post.ups\n",
    "        downvotes = upvotes - post.score\n",
    "        awards = post.all_awards if hasattr(post, 'all_awards') else []\n",
    "        award_names = [award['name'] for award in awards]\n",
    "        flair = post.link_flair_text if post.link_flair_text else \"No Flair\"\n",
    "        \n",
    "        post_data.append({\n",
    "            'Title': post.title,\n",
    "            'Body': post.selftext,\n",
    "            'Date': post_datetime,\n",
    "            'Upvotes': upvotes,\n",
    "            'Downvotes': downvotes,\n",
    "            'Comments': post.num_comments,\n",
    "            'Link': post.url,\n",
    "            'Awards': award_names,\n",
    "            'Flair': flair\n",
    "        })\n",
    "    \n",
    "    return post_data\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Named Entity Recognition\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Sentiment analysis\n",
    "sentiment_analyzer = pipeline('sentiment-analysis')\n",
    "def analyze_sentiment(text):\n",
    "    return sentiment_analyzer(text)\n",
    "\n",
    "# Topic Modeling with Top2Vec\n",
    "def get_topics(documents):\n",
    "    # Using Doc2Vec for initial processing\n",
    "    tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(documents)]\n",
    "    model = Doc2Vec(vector_size=100, min_count=2, epochs=40, workers=4)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    # Use Top2Vec for topic modeling\n",
    "    top2vec_model = Top2Vec(documents, embedding_model='doc2vec')\n",
    "    topics, topic_words, word_scores, topic_scores = top2vec_model.get_topics()\n",
    "    return topics, topic_words, word_scores, topic_scores\n",
    "\n",
    "# Event Timeline Plotting\n",
    "def plot_timeline(events):\n",
    "    event_dates = [datetime.strptime(event['date'], \"%Y-%m-%dT%H:%M:%SZ\") for event in events]\n",
    "    event_texts = [event['text'] for event in events]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(event_dates, [i for i in range(len(event_dates))], 'bo-')\n",
    "    \n",
    "    for i, text in enumerate(event_texts):\n",
    "        plt.text(event_dates[i], i, text, fontsize=9, verticalalignment='bottom', horizontalalignment='right')\n",
    "    \n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.title(\"Event Timeline\")\n",
    "    plt.xlabel(\"Date and Time\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main function to combine everything\n",
    "def summarize_event(topic):\n",
    "    # Fetch news and reddit data\n",
    "    news_articles = get_news(topic)\n",
    "    # reddit_posts = fetch_top_posts(topic)\n",
    "    \n",
    "    # Combine and preprocess data\n",
    "    combined_data = [article['description'] for article in news_articles if article['description']] \n",
    "    # + \\\n",
    "                    # [post['Body'] for post in reddit_posts if post['Body']]\n",
    "\n",
    "    cleaned_data = [preprocess_text(text) for text in combined_data]\n",
    "    \n",
    "    # Perform topic modeling\n",
    "    topics, topic_words, _, _ = get_topics(cleaned_data)\n",
    "    \n",
    "    # Extract entities and sentiment for each news and reddit post\n",
    "    events = []\n",
    "    for article in news_articles:\n",
    "        sentiment = analyze_sentiment(article['description'])\n",
    "        entities = extract_entities(article['description'])\n",
    "        events.append({\n",
    "            'text': article['title'],\n",
    "            'sentiment': sentiment,\n",
    "            'entities': entities,\n",
    "            'source': article['url'],\n",
    "            'date': article['publishedAt']\n",
    "        })\n",
    "    \n",
    "    for post in reddit_posts:\n",
    "        sentiment = analyze_sentiment(post['Body'])\n",
    "        entities = extract_entities(post['Body'])\n",
    "        events.append({\n",
    "            'text': post['Title'],\n",
    "            'sentiment': sentiment,\n",
    "            'entities': entities,\n",
    "            'source': post['Link'],\n",
    "            'date': post['Date']\n",
    "        })\n",
    "    \n",
    "    # Plot timeline\n",
    "    plot_timeline(events)\n",
    "    \n",
    "    # Print structured summary\n",
    "    print(\"Summary of Events:\")\n",
    "    for event in events:\n",
    "        print(f\"- {event['text']} (Source: {event['source']}, Date: {event['date']})\")\n",
    "        print(f\"  Sentiment: {event['sentiment'][0]['label']} ({event['sentiment'][0]['score']:.2f})\")\n",
    "        print(f\"  Key Entities: {', '.join([ent[0] for ent in event['entities']])}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter the event/topic: \")\n",
    "    summarize_event(topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
