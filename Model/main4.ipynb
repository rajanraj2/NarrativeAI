{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: you must first build vocabulary before training the model\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import praw\n",
    "import spacy\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from top2vec import Top2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up API keys (ensure security in a real application)\n",
    "NEWS_API_KEY = 'your_news_api_key'\n",
    "reddit = praw.Reddit(client_id='your_client_id',\n",
    "                     client_secret='your_client_secret',\n",
    "                     user_agent='event_summary_app')\n",
    "\n",
    "# Function to fetch news from NewsAPI\n",
    "def get_news(keyword, page_size=20):\n",
    "    url = f'https://newsapi.org/v2/everything?q={keyword}&apiKey={NEWS_API_KEY}&pageSize={page_size}'\n",
    "    response = requests.get(url)\n",
    "    articles = response.json().get('articles', [])\n",
    "    \n",
    "    news = []\n",
    "    for article in articles:\n",
    "        news.append({\n",
    "            'title': article['title'],\n",
    "            'description': article['description'],\n",
    "            'url': article['url'],\n",
    "            'publishedAt': article['publishedAt']\n",
    "        })\n",
    "    return news\n",
    "\n",
    "# Function to fetch top Reddit posts\n",
    "def fetch_top_posts(search_query, subreddit_name='all', limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    top_posts = subreddit.search(search_query, sort='top', limit=limit)\n",
    "    \n",
    "    post_data = []\n",
    "    for post in top_posts:\n",
    "        post_datetime = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        upvotes = post.ups\n",
    "        downvotes = upvotes - post.score\n",
    "        awards = post.all_awards if hasattr(post, 'all_awards') else []\n",
    "        award_names = [award['name'] for award in awards]\n",
    "        flair = post.link_flair_text if post.link_flair_text else \"No Flair\"\n",
    "        \n",
    "        post_data.append({\n",
    "            'Title': post.title,\n",
    "            'Body': post.selftext,\n",
    "            'Date': post_datetime,\n",
    "            'Upvotes': upvotes,\n",
    "            'Downvotes': downvotes,\n",
    "            'Comments': post.num_comments,\n",
    "            'Link': post.url,\n",
    "            'Awards': award_names,\n",
    "            'Flair': flair\n",
    "        })\n",
    "    \n",
    "    return post_data\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Named Entity Recognition\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Sentiment analysis with explicit model specification\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\", revision=\"af0f99b\")\n",
    "def analyze_sentiment(text):\n",
    "    return sentiment_analyzer(text)\n",
    "\n",
    "# Topic Modeling with Top2Vec\n",
    "def get_topics(documents):\n",
    "    # Prepare tagged data for Doc2Vec\n",
    "    tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(documents)]\n",
    "    \n",
    "    # Initialize and build vocabulary\n",
    "    model = Doc2Vec(vector_size=100, min_count=2, epochs=40, workers=4)\n",
    "    model.build_vocab(tagged_data)  # Ensure vocabulary is built\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    # Use Top2Vec for topic modeling\n",
    "    top2vec_model = Top2Vec(documents, embedding_model='universal-sentence-encoder')\n",
    "    \n",
    "    topics, topic_words, word_scores, topic_scores = top2vec_model.get_topics()\n",
    "    return topics, topic_words, word_scores, topic_scores\n",
    "\n",
    "# Event Timeline Plotting\n",
    "def plot_timeline(events):\n",
    "    event_dates = [datetime.strptime(event['date'], \"%Y-%m-%dT%H:%M:%SZ\") for event in events]\n",
    "    event_texts = [event['text'] for event in events]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(event_dates, [i for i in range(len(event_dates))], 'bo-')\n",
    "    \n",
    "    for i, text in enumerate(event_texts):\n",
    "        plt.text(event_dates[i], i, text, fontsize=9, verticalalignment='bottom', horizontalalignment='right')\n",
    "    \n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.title(\"Event Timeline\")\n",
    "    plt.xlabel(\"Date and Time\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main function to combine everything\n",
    "def summarize_event(topic):\n",
    "    try:\n",
    "        # Fetch news and reddit data\n",
    "        news_articles = get_news(topic)\n",
    "        # reddit_posts = fetch_top_posts(topic)\n",
    "        \n",
    "        # Combine and preprocess data\n",
    "        combined_data = [article['description'] for article in news_articles if article['description']] \n",
    "        # + \\\n",
    "                        # [post['Body'] for post in reddit_posts if post['Body']]\n",
    "\n",
    "        cleaned_data = [preprocess_text(text) for text in combined_data]\n",
    "        \n",
    "        # Perform topic modeling\n",
    "        topics, topic_words, _, _ = get_topics(cleaned_data)\n",
    "        \n",
    "        # Extract entities and sentiment for each news and reddit post\n",
    "        events = []\n",
    "        for article in news_articles:\n",
    "            sentiment = analyze_sentiment(article['description'])\n",
    "            entities = extract_entities(article['description'])\n",
    "            events.append({\n",
    "                'text': article['title'],\n",
    "                'sentiment': sentiment,\n",
    "                'entities': entities,\n",
    "                'source': article['url'],\n",
    "                'date': article['publishedAt']\n",
    "            })\n",
    "        \n",
    "        for post in reddit_posts:\n",
    "            sentiment = analyze_sentiment(post['Body'])\n",
    "            entities = extract_entities(post['Body'])\n",
    "            events.append({\n",
    "                'text': post['Title'],\n",
    "                'sentiment': sentiment,\n",
    "                'entities': entities,\n",
    "                'source': post['Link'],\n",
    "                'date': post['Date']\n",
    "            })\n",
    "        \n",
    "        # Plot timeline\n",
    "        plot_timeline(events)\n",
    "        \n",
    "        # Print structured summary\n",
    "        print(\"Summary of Events:\")\n",
    "        for event in events:\n",
    "            print(f\"- {event['text']} (Source: {event['source']}, Date: {event['date']})\")\n",
    "            print(f\"  Sentiment: {event['sentiment'][0]['label']} ({event['sentiment'][0]['score']:.2f})\")\n",
    "            print(f\"  Key Entities: {', '.join([ent[0] for ent in event['entities']])}\")\n",
    "            print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Input from user\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter the event/topic: \")\n",
    "    summarize_event(topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
